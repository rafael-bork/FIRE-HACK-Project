{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c700a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8814c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de colunas: 120\n",
      "  1. fid\n",
      "  2. fname\n",
      "  3. year\n",
      "  4. id\n",
      "  5. type\n",
      "  6. sdate\n",
      "  7. edate\n",
      "  8. inidoy\n",
      "  9. enddoy\n",
      " 10. source\n",
      " 11. zp_link\n",
      " 12. burn_perio\n",
      " 13. area\n",
      " 14. growth_rat\n",
      " 15. ros_i\n",
      " 16. ros_p\n",
      " 17. spdir_i\n",
      " 18. spdir_p\n",
      " 19. int_i\n",
      " 20. int_p\n",
      " 21. duration_i\n",
      " 22. duration_p\n",
      " 23. qc\n",
      " 24. 1_3y_fir_p\n",
      " 25. 3_8y_fir_p\n",
      " 26. 8_ny_fir_p\n",
      " 27. elev_av\n",
      " 28. aspect_av\n",
      " 29. landform\n",
      " 30. fuel_model\n",
      " 31. f_load_av\n",
      " 32. land_use\n",
      " 33. land_use_d\n",
      " 34. gp_m2s2_av\n",
      " 35. CBH_m_av\n",
      " 36. HigCC_p_av\n",
      " 37. LowCC_p_av\n",
      " 38. MidCC_p_av\n",
      " 39. TotCC_p_av\n",
      " 40. BLH_m_av\n",
      " 41. Cape_av\n",
      " 42. Cin_av\n",
      " 43. sW_7_av\n",
      " 44. sW_28_av\n",
      " 45. sW_100_av\n",
      " 46. sW_289_av\n",
      " 47. DC_12h_av\n",
      " 48. FFMC_12h_a\n",
      " 49. FWI_12h_av\n",
      " 50. t_2m_C_av\n",
      " 51. d_2m_C_av\n",
      " 52. sP_hPa_av\n",
      " 53. wv10_kh_av\n",
      " 54. wdir10_av\n",
      " 55. wv_Fb_av\n",
      " 56. wdir_Fb_av\n",
      " 57. wv100_k_av\n",
      " 58. wdir100_av\n",
      " 59. rh_2m_av\n",
      " 60. VPD_Pa_av\n",
      " 61. dfmc_av\n",
      " 62. sW_1m_av\n",
      " 63. sW_3m_av\n",
      " 64. LCL_hPa_av\n",
      " 65. LCL_m_av\n",
      " 66. HDW_av\n",
      " 67. Haines_av\n",
      " 68. wSv_9_av\n",
      " 69. wSdir_9_av\n",
      " 70. wSv_7_av\n",
      " 71. wSdir_7_av\n",
      " 72. wSv_5_av\n",
      " 73. wSdir_5_av\n",
      " 74. wSv_1_av\n",
      " 75. wSdir_1_av\n",
      " 76. gT_s_9_av\n",
      " 77. gT_9_8_av\n",
      " 78. gT_8_7_av\n",
      " 79. gT_7_5_av\n",
      " 80. gT_5_3_av\n",
      " 81. CMLG_av\n",
      " 82. LFC_hPa_av\n",
      " 83. CCL_hPa_av\n",
      " 84. EL_m_av\n",
      " 85. VentIdx_av\n",
      " 86. LiftIdx_av\n",
      " 87. gp_950_av\n",
      " 88. gp_850_av\n",
      " 89. gp_700_av\n",
      " 90. gp_500_av\n",
      " 91. gp_300_av\n",
      " 92. rh_950_av\n",
      " 93. rh_850_av\n",
      " 94. rh_700_av\n",
      " 95. rh_500_av\n",
      " 96. rh_300_av\n",
      " 97. t_950_av\n",
      " 98. t_850_av\n",
      " 99. t_700_av\n",
      "100. t_500_av\n",
      "101. t_300_av\n",
      "102. wv_950_av\n",
      "103. wv_850_av\n",
      "104. wv_700_av\n",
      "105. wv_500_av\n",
      "106. wv_300_av\n",
      "107. wdi_950_av\n",
      "108. wdi_850_av\n",
      "109. wdi_700_av\n",
      "110. wdi_500_av\n",
      "111. wdi_300_av\n",
      "112. vwv_950_av\n",
      "113. vwv_850_av\n",
      "114. vwv_700_av\n",
      "115. vwv_500_av\n",
      "116. vwv_300_av\n",
      "117. BLH_m_rt\n",
      "118. Recirc\n",
      "119. CircVar\n",
      "120. geometry\n"
     ]
    }
   ],
   "source": [
    "shp = gpd.read_file(r\"..\\..\\Data\\Interim\\PT-FireSprd_v2.1\\L2_FireBehavior\\PT-FireProg_v2.1_L2_p_meteo.shp\")\n",
    "\n",
    "print(f\"Total de colunas: {len(shp.columns)}\")\n",
    "for i, coluna in enumerate(shp.columns, 1):\n",
    "    print(f\"{i:3d}. {coluna}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4110ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dias3\\AppData\\Local\\Temp\\ipykernel_864\\2999221567.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  shp_combined[\"sdate\"] = pd.to_datetime(shp_combined[\"sdate\"], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Lags temporais adicionados com base na feature ativa há X horas.\n"
     ]
    }
   ],
   "source": [
    "columns_to_lag = [\"ros_p\"]\n",
    "n_lags = 1\n",
    "shp_combined = shp.sort_values([\"fname\", \"zp_link\", \"sdate\"]).reset_index(drop=True)\n",
    "\n",
    "# garantir datetime\n",
    "shp_combined[\"sdate\"] = pd.to_datetime(shp_combined[\"sdate\"], errors='coerce')\n",
    "shp_combined[\"edate\"] = pd.to_datetime(shp_combined[\"edate\"], errors='coerce')\n",
    "\n",
    "for col in columns_to_lag:\n",
    "    if col not in shp_combined.columns:\n",
    "        print(f\"⚠️ Coluna '{col}' não encontrada. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        lag_col_name = f\"{col}_lg{lag}\"\n",
    "        shp_combined[lag_col_name] = np.nan\n",
    "\n",
    "        # processa por grupo (fname, zplink)\n",
    "        for (fname, zplink), group in shp_combined.groupby([\"fname\", \"zp_link\"]):\n",
    "            group = group.sort_values(\"sdate\")\n",
    "            group_idx = group.index\n",
    "\n",
    "            for idx in group_idx:\n",
    "                current_time = shp_combined.loc[idx, \"sdate\"]  # início do evento atual\n",
    "                target_time = current_time - pd.Timedelta(hours=lag)\n",
    "\n",
    "                # procurar a feature anterior que estava ativa no instante target_time\n",
    "                mask = (group[\"sdate\"] <= target_time) & (group[\"edate\"] > target_time)\n",
    "\n",
    "                if mask.any():\n",
    "                    active_row = group.loc[mask].iloc[-1]\n",
    "                    shp_combined.at[idx, lag_col_name] = active_row[col]\n",
    "                else:\n",
    "                    shp_combined.at[idx, lag_col_name] = np.nan\n",
    "\n",
    "print(\"✅ Lags temporais adicionados com base na feature ativa há X horas.\")\n",
    "\n",
    "shp_lags = shp_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a7b7904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying calculation for 'Agueda_08082016':\n",
      "Empty DataFrame\n",
      "Columns: [fname, sdate, f_start]\n",
      "Index: []\n",
      "\n",
      "Verifying calculation for 'Gouveia_10082015':\n",
      "Empty DataFrame\n",
      "Columns: [fname, sdate, f_start]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def calculate_fire_durations(df):\n",
    "    \"\"\"\n",
    "    Calculates 'duration' (time since fire start) and lag features \n",
    "    (time differences between consecutive observations).\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # 1. Convert sdate to datetime\n",
    "    df_temp['sdate'] = pd.to_datetime(df_temp['sdate'], errors='coerce')\n",
    "    \n",
    "    # 2. Sort by fire name and date\n",
    "    df_temp = df_temp.sort_values(by=['fname', 'sdate'])\n",
    "    \n",
    "    # 3. Calculate 'duration' (time since the start of the fire)\n",
    "    fire_start_times = df_temp.groupby('fname')['sdate'].transform('min')\n",
    "    df_temp['f_start'] = (df_temp['sdate'] - fire_start_times).dt.total_seconds() / 60\n",
    "    \n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "# Apply the corrected function\n",
    "shp_processed = calculate_fire_durations(shp_lags)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nVerifying calculation for 'Agueda_08082016':\")\n",
    "print(shp_processed[shp_processed['f_start'] == 'Agueda_08082016'][\n",
    "    ['fname', 'sdate', 'f_start']\n",
    "].head(10))\n",
    "\n",
    "print(\"\\nVerifying calculation for 'Gouveia_10082015':\")\n",
    "print(shp_processed[shp_processed['f_start'] == 'Gouveia_10082015'][\n",
    "    ['fname', 'sdate', 'f_start']\n",
    "].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4893de57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dias3\\AppData\\Local\\Temp\\ipykernel_864\\2116938553.py:28: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = row['sdate'].ceil('H')  # next full hour\n",
      "C:\\Users\\dias3\\AppData\\Local\\Temp\\ipykernel_864\\2116938553.py:29: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  e = row['edate'].floor('H') # last full hour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire front rank distribution:\n",
      "fire_rank\n",
      "-4       3\n",
      "-3     254\n",
      "-2      14\n",
      "-1    1640\n",
      " 1     910\n",
      " 2     267\n",
      " 3     267\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total rows: 3355\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create fire front ranking system:\n",
    "    -4: Other missing values\n",
    "    -3: Partially overlapping intervals\n",
    "    -2: No meteorological data because of short time interval\n",
    "    -1: Missing sdate or edate\n",
    "    1: Single fire front for specific fire at specific time\n",
    "    2: Multiple fire fronts - highest ROS_p\n",
    "    3: Multiple fire fronts - lowest ROS_p\n",
    "\"\"\"\n",
    "shp_processed['fire_rank'] = 1\n",
    "\n",
    "# Converter colunas de data para datetime se forem strings\n",
    "if shp_processed['sdate'].dtype == 'object':\n",
    "    shp_processed['sdate'] = pd.to_datetime(shp_processed['sdate'])\n",
    "if shp_processed['edate'].dtype == 'object':\n",
    "    shp_processed['edate'] = pd.to_datetime(shp_processed['edate'])\n",
    "\n",
    "# -1: Missing sdate or edate\n",
    "missing_mask = shp_processed['sdate'].isna() | shp_processed['edate'].isna()\n",
    "shp_processed.loc[missing_mask, 'fire_rank'] = -1\n",
    "\n",
    "# -2: No meteorological data because of short time interval\n",
    "def interval_has_full_hour(row):\n",
    "    if pd.isna(row['sdate']) or pd.isna(row['edate']):\n",
    "        return False\n",
    "    # Generate range of hours\n",
    "    s = row['sdate'].ceil('H')  # next full hour\n",
    "    e = row['edate'].floor('H') # last full hour\n",
    "    return s <= e\n",
    "\n",
    "mask_no_full_hour = (~missing_mask) & (~shp_processed.apply(interval_has_full_hour, axis=1))\n",
    "shp_processed.loc[mask_no_full_hour, 'fire_rank'] = -2\n",
    "\n",
    "# -4: Other missing values\n",
    "mask_missing_meteo = shp_processed['t_2m_C_av'].isna() & (shp_processed['ros_p'] != -1) & (shp_processed['fire_rank'] > 0)\n",
    "shp_processed.loc[mask_missing_meteo, 'fire_rank'] = -4\n",
    "\n",
    "# -3: Partially overlapping intervals\n",
    "valid_mask = (shp_processed['fire_rank'] == 1)\n",
    "\n",
    "# Ordenar por fname e sdate para verificar sobreposições\n",
    "shp_processed_sorted = shp_processed[valid_mask].sort_values(['fname', 'sdate'])\n",
    "\n",
    "for fname, group in shp_processed_sorted.groupby('fname'):\n",
    "    if len(group) > 1:\n",
    "        # Verificar sobreposição entre intervalos consecutivos\n",
    "        for i in range(len(group) - 1):\n",
    "            current = group.iloc[i]\n",
    "            next_row = group.iloc[i + 1]\n",
    "            \n",
    "            # Verificar se há sobreposição parcial\n",
    "            # Sobreposição ocorre se: current_edate > next_sdate\n",
    "            # E NÃO são exatamente o mesmo intervalo (para permitir ranks 2 e 3)\n",
    "            if (current['edate'] > next_row['sdate']) and not (\n",
    "                current['sdate'] == next_row['sdate'] and current['edate'] == next_row['edate']\n",
    "            ):\n",
    "                # Marcar ambas as progressões sobrepostas como -3\n",
    "                shp_processed.loc[current.name, 'fire_rank'] = -3\n",
    "                shp_processed.loc[next_row.name, 'fire_rank'] = -3\n",
    "\n",
    "# 2 e 3: Multiple fire fronts - highest and lowest ROS_p\n",
    "valid_mask = (shp_processed['fire_rank'] == 1)\n",
    "for (fname, sdate, edate), group in shp_processed[valid_mask].groupby(['fname', 'sdate', 'edate']):\n",
    "    valid_rows = group[(group['type'] == 'p') & (group['ros_p'] > 0)]\n",
    "    if len(valid_rows) > 1:\n",
    "        max_ros_idx = valid_rows['ros_p'].idxmax()\n",
    "        min_ros_idx = valid_rows['ros_p'].idxmin()\n",
    "        shp_processed.loc[max_ros_idx, 'fire_rank'] = 2\n",
    "        shp_processed.loc[min_ros_idx, 'fire_rank'] = 3\n",
    "\n",
    "# Display results\n",
    "rank_counts = shp_processed['fire_rank'].value_counts().sort_index()\n",
    "print(\"Fire front rank distribution:\")\n",
    "print(rank_counts)\n",
    "print(f\"\\nTotal rows: {len(shp_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cf40b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. fid\n",
      "  2. fname\n",
      "  3. year\n",
      "  4. id\n",
      "  5. type\n",
      "  6. sdate\n",
      "  7. edate\n",
      "  8. inidoy\n",
      "  9. enddoy\n",
      " 10. source\n",
      " 11. zp_link\n",
      " 12. burn_perio\n",
      " 13. area\n",
      " 14. growth_rat\n",
      " 15. ros_i\n",
      " 16. ros_p\n",
      " 17. spdir_i\n",
      " 18. spdir_p\n",
      " 19. int_i\n",
      " 20. int_p\n",
      " 21. duration_i\n",
      " 22. duration_p\n",
      " 23. qc\n",
      " 24. 1_3y_fir_p\n",
      " 25. 3_8y_fir_p\n",
      " 26. 8_ny_fir_p\n",
      " 27. elev_av\n",
      " 28. aspect_av\n",
      " 29. landform\n",
      " 30. fuel_model\n",
      " 31. f_load_av\n",
      " 32. land_use\n",
      " 33. land_use_d\n",
      " 34. gp_m2s2_av\n",
      " 35. CBH_m_av\n",
      " 36. HigCC_p_av\n",
      " 37. LowCC_p_av\n",
      " 38. MidCC_p_av\n",
      " 39. TotCC_p_av\n",
      " 40. BLH_m_av\n",
      " 41. Cape_av\n",
      " 42. Cin_av\n",
      " 43. sW_7_av\n",
      " 44. sW_28_av\n",
      " 45. sW_100_av\n",
      " 46. sW_289_av\n",
      " 47. DC_12h_av\n",
      " 48. FFMC_12h_a\n",
      " 49. FWI_12h_av\n",
      " 50. t_2m_C_av\n",
      " 51. d_2m_C_av\n",
      " 52. sP_hPa_av\n",
      " 53. wv10_kh_av\n",
      " 54. wdir10_av\n",
      " 55. wv_Fb_av\n",
      " 56. wdir_Fb_av\n",
      " 57. wv100_k_av\n",
      " 58. wdir100_av\n",
      " 59. rh_2m_av\n",
      " 60. VPD_Pa_av\n",
      " 61. dfmc_av\n",
      " 62. sW_1m_av\n",
      " 63. sW_3m_av\n",
      " 64. LCL_hPa_av\n",
      " 65. LCL_m_av\n",
      " 66. HDW_av\n",
      " 67. Haines_av\n",
      " 68. wSv_9_av\n",
      " 69. wSdir_9_av\n",
      " 70. wSv_7_av\n",
      " 71. wSdir_7_av\n",
      " 72. wSv_5_av\n",
      " 73. wSdir_5_av\n",
      " 74. wSv_1_av\n",
      " 75. wSdir_1_av\n",
      " 76. gT_s_9_av\n",
      " 77. gT_9_8_av\n",
      " 78. gT_8_7_av\n",
      " 79. gT_7_5_av\n",
      " 80. gT_5_3_av\n",
      " 81. CMLG_av\n",
      " 82. LFC_hPa_av\n",
      " 83. CCL_hPa_av\n",
      " 84. EL_m_av\n",
      " 85. VentIdx_av\n",
      " 86. LiftIdx_av\n",
      " 87. gp_950_av\n",
      " 88. gp_850_av\n",
      " 89. gp_700_av\n",
      " 90. gp_500_av\n",
      " 91. gp_300_av\n",
      " 92. rh_950_av\n",
      " 93. rh_850_av\n",
      " 94. rh_700_av\n",
      " 95. rh_500_av\n",
      " 96. rh_300_av\n",
      " 97. t_950_av\n",
      " 98. t_850_av\n",
      " 99. t_700_av\n",
      "100. t_500_av\n",
      "101. t_300_av\n",
      "102. wv_950_av\n",
      "103. wv_850_av\n",
      "104. wv_700_av\n",
      "105. wv_500_av\n",
      "106. wv_300_av\n",
      "107. wdi_950_av\n",
      "108. wdi_850_av\n",
      "109. wdi_700_av\n",
      "110. wdi_500_av\n",
      "111. wdi_300_av\n",
      "112. vwv_950_av\n",
      "113. vwv_850_av\n",
      "114. vwv_700_av\n",
      "115. vwv_500_av\n",
      "116. vwv_300_av\n",
      "117. BLH_m_rt\n",
      "118. Recirc\n",
      "119. CircVar\n",
      "120. geometry\n",
      "121. ros_p_lg1\n",
      "122. f_start\n",
      "123. fire_rank\n"
     ]
    }
   ],
   "source": [
    "for i, coluna in enumerate(shp_processed.columns, 1):\n",
    "    print(f\"{i:3d}. {coluna}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0795c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_processed['HigCC_p_av'] = shp_processed['HigCC_p_av'] * 100\n",
    "shp_processed['MidCC_p_av'] = shp_processed['MidCC_p_av'] * 100\n",
    "shp_processed['LowCC_p_av'] = shp_processed['LowCC_p_av'] * 100\n",
    "shp_processed['TotCC_p_av'] = shp_processed['TotCC_p_av'] * 100\n",
    "\n",
    "shp_processed = shp_processed.drop(columns=['wv_Fb_av', 'wdir_Fb_av'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd9110c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faltantes no DataFrame: []\n",
      "Colunas extras (não ordenadas): []\n",
      "Reordenação concluída!\n"
     ]
    }
   ],
   "source": [
    "# mantém a ordem original até 'qc'\n",
    "originais_ate_qc = [\n",
    "    'fid','fname','year','id','type','sdate','edate','inidoy','enddoy',\n",
    "    'source','zp_link','burn_perio','area','growth_rat','ros_i','ros_p',\n",
    "    'spdir_i','spdir_p','int_i','int_p','duration_i','duration_p','qc'\n",
    "]\n",
    "\n",
    "\n",
    "# nova ordem racional após qc\n",
    "novas_ordenadas = [\n",
    "\n",
    "    # 2. Topography\n",
    "    'elev_av','aspect_av','landform',\n",
    "\n",
    "    # 3. Land Use\n",
    "    'land_use','land_use_d',\n",
    "\n",
    "    # 1. Fuel\n",
    "    '1_3y_fir_p','3_8y_fir_p','8_ny_fir_p','fuel_model','f_load_av',\n",
    "\n",
    "    # 5. Soil Moisture\n",
    "    'sW_1m_av','sW_3m_av','sW_7_av','sW_28_av','sW_100_av','sW_289_av',\n",
    "\n",
    "    # 4. Surface Meteorology\n",
    "    't_2m_C_av','d_2m_C_av','rh_2m_av','VPD_Pa_av','sP_hPa_av', 'gp_m2s2_av',\n",
    "    'dfmc_av','HDW_av','Haines_av',\n",
    "\n",
    "    # 19. Fire Weather Indexes\n",
    "    'FWI_12h_av', 'DC_12h_av', 'FFMC_12h_a',\n",
    "\n",
    "    # 10. Wind - Surface & 100m\n",
    "    'wv10_kh_av','wdir10_av','wv100_k_av','wdir100_av',\n",
    "\n",
    "    'Recirc','CircVar',\n",
    "\n",
    "    # 11. Horizontal Wind Speeds (levels)\n",
    "    'wv_950_av','wv_850_av','wv_700_av','wv_500_av','wv_300_av',\n",
    "\n",
    "    # 12. Horizontal Wind Directions (levels)\n",
    "    'wdi_950_av','wdi_850_av','wdi_700_av','wdi_500_av','wdi_300_av',\n",
    "\n",
    "    # 13. Vertical Wind Velocity\n",
    "    'vwv_950_av','vwv_850_av','vwv_700_av','vwv_500_av','vwv_300_av',\n",
    "\n",
    "    # 14. Relative Humidity Levels\n",
    "    'rh_950_av','rh_850_av','rh_700_av','rh_500_av','rh_300_av',\n",
    "\n",
    "    # 15. Temperature Levels\n",
    "    't_950_av','t_850_av','t_700_av','t_500_av','t_300_av',\n",
    "\n",
    "    # 16. Geopotential\n",
    "    'gp_950_av','gp_850_av','gp_700_av','gp_500_av','gp_300_av',\n",
    "\n",
    "    # 8. Temperature Gradients\n",
    "    'gT_s_9_av','gT_9_8_av','gT_8_7_av','gT_7_5_av','gT_5_3_av',\n",
    "\n",
    "    # 9. Wind Shear\n",
    "    'wSv_9_av','wSdir_9_av','wSv_7_av','wSdir_7_av',\n",
    "    'wSv_5_av','wSdir_5_av','wSv_1_av','wSdir_1_av',\n",
    "\n",
    "    # 6. Clouds & BLH\n",
    "    'CBH_m_av','HigCC_p_av','LowCC_p_av','MidCC_p_av','TotCC_p_av',\n",
    "\n",
    "    # 7. Convection & Lifting\n",
    "    'Cape_av','Cin_av', 'BLH_m_av', 'BLH_m_rt', \n",
    "    'LCL_hPa_av','LCL_m_av',\n",
    "    'LFC_hPa_av','CCL_hPa_av','EL_m_av','LiftIdx_av',\n",
    "    'VentIdx_av','CMLG_av',\n",
    "\n",
    "    # 17. Additional Fire Behaviour\n",
    "    'ros_p_lg1','f_start','fire_rank',\n",
    "\n",
    "    # 18. Geometry\n",
    "    'geometry'\n",
    "]\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# construir lista final\n",
    "# ----------------------------\n",
    "\n",
    "todas_cols = originais_ate_qc + novas_ordenadas\n",
    "\n",
    "# verificar se existem colunas faltantes ou extras\n",
    "faltantes = [c for c in todas_cols if c not in shp_processed.columns]\n",
    "extras = [c for c in shp_processed.columns if c not in todas_cols]\n",
    "\n",
    "print(\"Faltantes no DataFrame:\", faltantes)\n",
    "print(\"Colunas extras (não ordenadas):\", extras)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# reordenar o DataFrame\n",
    "# ----------------------------\n",
    "shp_processed = shp_processed[todas_cols]\n",
    "\n",
    "print(\"Reordenação concluída!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57a47ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_processed['sdate'] = shp_processed['sdate'].astype(str)\n",
    "shp_processed['edate'] = shp_processed['edate'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44ac33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(r'..\\..\\Data\\Processed\\PT-FireSprd_v2.1\\L2_FireBehavior', exist_ok=True)\n",
    "shp_processed.to_file(r'..\\..\\Data\\Processed\\PT-FireSprd_v2.1\\L2_FireBehavior\\PT-FireProg_v2.1_L2_final.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
