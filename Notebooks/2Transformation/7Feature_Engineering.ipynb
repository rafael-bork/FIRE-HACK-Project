{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c700a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8814c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de colunas: 117\n",
      "  1. fid\n",
      "  2. fname\n",
      "  3. year\n",
      "  4. id\n",
      "  5. type\n",
      "  6. sdate\n",
      "  7. edate\n",
      "  8. inidoy\n",
      "  9. enddoy\n",
      " 10. source\n",
      " 11. zp_link\n",
      " 12. burn_perio\n",
      " 13. area\n",
      " 14. growth_rat\n",
      " 15. ros_i\n",
      " 16. ros_p\n",
      " 17. spdir_i\n",
      " 18. spdir_p\n",
      " 19. int_i\n",
      " 20. int_p\n",
      " 21. duration_i\n",
      " 22. duration_p\n",
      " 23. qc\n",
      " 24. 1_3y_fir_p\n",
      " 25. 3_8y_fir_p\n",
      " 26. 8_ny_fir_p\n",
      " 27. elev_av\n",
      " 28. aspect_av\n",
      " 29. landform\n",
      " 30. fuel_model\n",
      " 31. f_load_av\n",
      " 32. land_use\n",
      " 33. land_use_d\n",
      " 34. CBH_m_av\n",
      " 35. HigCC_p_av\n",
      " 36. LowCC_p_av\n",
      " 37. MidCC_p_av\n",
      " 38. TotCC_p_av\n",
      " 39. BLH_m_av\n",
      " 40. Cape_av\n",
      " 41. Cin_av\n",
      " 42. sW_7_av\n",
      " 43. sW_28_av\n",
      " 44. sW_100_av\n",
      " 45. sW_289_av\n",
      " 46. gp_m2s2_av\n",
      " 47. t_2m_C_av\n",
      " 48. d_2m_C_av\n",
      " 49. sP_hPa_av\n",
      " 50. wv10_kh_av\n",
      " 51. wdir10_av\n",
      " 52. wv100_k_av\n",
      " 53. wdir100_av\n",
      " 54. rh_2m_av\n",
      " 55. VPD_Pa_av\n",
      " 56. dfmc_av\n",
      " 57. sW_1m_av\n",
      " 58. sW_3m_av\n",
      " 59. LCL_hPa_av\n",
      " 60. LCL_m_av\n",
      " 61. HDW_av\n",
      " 62. Haines_av\n",
      " 63. wSv_9_av\n",
      " 64. wSdir_9_av\n",
      " 65. wSv_7_av\n",
      " 66. wSdir_7_av\n",
      " 67. wSv_5_av\n",
      " 68. wSdir_5_av\n",
      " 69. wSv_1_av\n",
      " 70. wSdir_1_av\n",
      " 71. gT_s_9_av\n",
      " 72. gT_9_8_av\n",
      " 73. gT_8_7_av\n",
      " 74. gT_7_5_av\n",
      " 75. gT_5_3_av\n",
      " 76. CMLG_av\n",
      " 77. LFC_hPa_av\n",
      " 78. CCL_hPa_av\n",
      " 79. EL_m_av\n",
      " 80. VentIdx_av\n",
      " 81. LiftIdx_av\n",
      " 82. gp_950_av\n",
      " 83. gp_850_av\n",
      " 84. gp_700_av\n",
      " 85. gp_500_av\n",
      " 86. gp_300_av\n",
      " 87. rh_950_av\n",
      " 88. rh_850_av\n",
      " 89. rh_700_av\n",
      " 90. rh_500_av\n",
      " 91. rh_300_av\n",
      " 92. t_950_av\n",
      " 93. t_850_av\n",
      " 94. t_700_av\n",
      " 95. t_500_av\n",
      " 96. t_300_av\n",
      " 97. wv_950_av\n",
      " 98. wv_850_av\n",
      " 99. wv_700_av\n",
      "100. wv_500_av\n",
      "101. wv_300_av\n",
      "102. wdi_950_av\n",
      "103. wdi_850_av\n",
      "104. wdi_700_av\n",
      "105. wdi_500_av\n",
      "106. wdi_300_av\n",
      "107. vwv_950_av\n",
      "108. vwv_850_av\n",
      "109. vwv_700_av\n",
      "110. vwv_500_av\n",
      "111. vwv_300_av\n",
      "112. wv10_av\n",
      "113. wv100_av\n",
      "114. BLH_m_rt\n",
      "115. Recirc\n",
      "116. CircVar\n",
      "117. geometry\n"
     ]
    }
   ],
   "source": [
    "shp = gpd.read_file(r\"../../Data/Interim/PT-FireSprd_v2.1/L2_FireBehavior/PT-FireProg_v2.1_L2_p_meteo.shp\")\n",
    "\n",
    "print(f\"Total de colunas: {len(shp.columns)}\")\n",
    "for i, coluna in enumerate(shp.columns, 1):\n",
    "    print(f\"{i:3d}. {coluna}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4110ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kd/4r8kz2pd215cjlt29s2n99vh0000gn/T/ipykernel_16970/2999221567.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  shp_combined[\"sdate\"] = pd.to_datetime(shp_combined[\"sdate\"], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Lags temporais adicionados com base na feature ativa há X horas.\n"
     ]
    }
   ],
   "source": [
    "columns_to_lag = [\"ros_p\"]\n",
    "n_lags = 1\n",
    "shp_combined = shp.sort_values([\"fname\", \"zp_link\", \"sdate\"]).reset_index(drop=True)\n",
    "\n",
    "# garantir datetime\n",
    "shp_combined[\"sdate\"] = pd.to_datetime(shp_combined[\"sdate\"], errors='coerce')\n",
    "shp_combined[\"edate\"] = pd.to_datetime(shp_combined[\"edate\"], errors='coerce')\n",
    "\n",
    "for col in columns_to_lag:\n",
    "    if col not in shp_combined.columns:\n",
    "        print(f\"⚠️ Coluna '{col}' não encontrada. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        lag_col_name = f\"{col}_lg{lag}\"\n",
    "        shp_combined[lag_col_name] = np.nan\n",
    "\n",
    "        # processa por grupo (fname, zplink)\n",
    "        for (fname, zplink), group in shp_combined.groupby([\"fname\", \"zp_link\"]):\n",
    "            group = group.sort_values(\"sdate\")\n",
    "            group_idx = group.index\n",
    "\n",
    "            for idx in group_idx:\n",
    "                current_time = shp_combined.loc[idx, \"sdate\"]  # início do evento atual\n",
    "                target_time = current_time - pd.Timedelta(hours=lag)\n",
    "\n",
    "                # procurar a feature anterior que estava ativa no instante target_time\n",
    "                mask = (group[\"sdate\"] <= target_time) & (group[\"edate\"] > target_time)\n",
    "\n",
    "                if mask.any():\n",
    "                    active_row = group.loc[mask].iloc[-1]\n",
    "                    shp_combined.at[idx, lag_col_name] = active_row[col]\n",
    "                else:\n",
    "                    shp_combined.at[idx, lag_col_name] = np.nan\n",
    "\n",
    "print(\"✅ Lags temporais adicionados com base na feature ativa há X horas.\")\n",
    "\n",
    "shp_lags = shp_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a7b7904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nVerifying calculation for 'Agueda_08082016':\n",
      "Empty DataFrame\n",
      "Columns: [fname, sdate, f_start]\n",
      "Index: []\n",
      "/nVerifying calculation for 'Gouveia_10082015':\n",
      "Empty DataFrame\n",
      "Columns: [fname, sdate, f_start]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "def calculate_fire_durations(df):\n",
    "    \"\"\"\n",
    "    Calculates 'duration' (time since fire start) and lag features \n",
    "    (time differences between consecutive observations).\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # 1. Convert sdate to datetime\n",
    "    df_temp['sdate'] = pd.to_datetime(df_temp['sdate'], errors='coerce')\n",
    "    \n",
    "    # 2. Sort by fire name and date\n",
    "    df_temp = df_temp.sort_values(by=['fname', 'sdate'])\n",
    "    \n",
    "    # 3. Calculate 'duration' (time since the start of the fire)\n",
    "    fire_start_times = df_temp.groupby('fname')['sdate'].transform('min')\n",
    "    df_temp['f_start'] = (df_temp['sdate'] - fire_start_times).dt.total_seconds() / 60\n",
    "    \n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "# Apply the corrected function\n",
    "shp_processed = calculate_fire_durations(shp_lags)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"/nVerifying calculation for 'Agueda_08082016':\")\n",
    "print(shp_processed[shp_processed['f_start'] == 'Agueda_08082016'][\n",
    "    ['fname', 'sdate', 'f_start']\n",
    "].head(10))\n",
    "\n",
    "print(\"/nVerifying calculation for 'Gouveia_10082015':\")\n",
    "print(shp_processed[shp_processed['f_start'] == 'Gouveia_10082015'][\n",
    "    ['fname', 'sdate', 'f_start']\n",
    "].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4893de57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kd/4r8kz2pd215cjlt29s2n99vh0000gn/T/ipykernel_16970/3493942893.py:22: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = row['sdate'].ceil('H')  # next full hour\n",
      "/var/folders/kd/4r8kz2pd215cjlt29s2n99vh0000gn/T/ipykernel_16970/3493942893.py:23: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  e = row['edate'].floor('H') # last full hour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire front rank distribution:\n",
      "fire_rank\n",
      "-3       3\n",
      "-2      14\n",
      "-1    1640\n",
      " 1    1020\n",
      " 2     339\n",
      " 3     339\n",
      "Name: count, dtype: int64\n",
      "/nTotal rows: 3355\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create fire front ranking system:\n",
    "    -3: Other missing values\n",
    "    -2: No meteorological data because of short time interval\n",
    "    -1: Missing sdate or edate\n",
    "    1: Single fire front for specific fire at specific time\n",
    "    2: Multiple fire fronts - highest ROS_p\n",
    "    3: Multiple fire fronts - lowest ROS_p\n",
    "\"\"\"\n",
    "shp_processed['fire_rank'] = 1\n",
    "\n",
    "# Handle missing dates\n",
    "missing_mask = shp_processed['sdate'].isna() | shp_processed['edate'].isna()\n",
    "shp_processed.loc[missing_mask, 'fire_rank'] = -1\n",
    "\n",
    "\n",
    "# Handle intervals that don't contain exact hour (-2)\n",
    "def interval_has_full_hour(row):\n",
    "    if pd.isna(row['sdate']) or pd.isna(row['edate']):\n",
    "        return False\n",
    "    # Generate range of hours\n",
    "    s = row['sdate'].ceil('H')  # next full hour\n",
    "    e = row['edate'].floor('H') # last full hour\n",
    "    return s <= e\n",
    "\n",
    "mask_no_full_hour = (~missing_mask) & (~shp_processed.apply(interval_has_full_hour, axis=1))\n",
    "shp_processed.loc[mask_no_full_hour, 'fire_rank'] = -2\n",
    "\n",
    "# Handle missing meteorological data (-3)\n",
    "mask_missing_meteo = shp_processed['t_2m_C_av'].isna() & (shp_processed['ros_p'] != -1) & (shp_processed['fire_rank'] != -2)\n",
    "shp_processed.loc[mask_missing_meteo, 'fire_rank'] = -3\n",
    "\n",
    "# Process valid rows\n",
    "valid_mask = (~missing_mask) & (~mask_no_full_hour)\n",
    "for (fname, sdate), group in shp_processed[valid_mask].groupby(['fname', 'sdate']):\n",
    "    valid_rows = group[(group['type'] == 'p') & (group['ros_p'] > 0) & (group['fire_rank'] == 1)]\n",
    "    if len(valid_rows) > 1:\n",
    "        max_ros_idx = valid_rows['ros_p'].idxmax()\n",
    "        min_ros_idx = valid_rows['ros_p'].idxmin()\n",
    "        shp_processed.loc[max_ros_idx, 'fire_rank'] = 2\n",
    "        shp_processed.loc[min_ros_idx, 'fire_rank'] = 3\n",
    "\n",
    "# Display results\n",
    "rank_counts = shp_processed['fire_rank'].value_counts().sort_index()\n",
    "print(\"Fire front rank distribution:\")\n",
    "print(rank_counts)\n",
    "print(f\"/nTotal rows: {len(shp_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cf40b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. fid\n",
      "  2. fname\n",
      "  3. year\n",
      "  4. id\n",
      "  5. type\n",
      "  6. sdate\n",
      "  7. edate\n",
      "  8. inidoy\n",
      "  9. enddoy\n",
      " 10. source\n",
      " 11. zp_link\n",
      " 12. burn_perio\n",
      " 13. area\n",
      " 14. growth_rat\n",
      " 15. ros_i\n",
      " 16. ros_p\n",
      " 17. spdir_i\n",
      " 18. spdir_p\n",
      " 19. int_i\n",
      " 20. int_p\n",
      " 21. duration_i\n",
      " 22. duration_p\n",
      " 23. qc\n",
      " 24. 1_3y_fir_p\n",
      " 25. 3_8y_fir_p\n",
      " 26. 8_ny_fir_p\n",
      " 27. elev_av\n",
      " 28. aspect_av\n",
      " 29. landform\n",
      " 30. fuel_model\n",
      " 31. f_load_av\n",
      " 32. land_use\n",
      " 33. land_use_d\n",
      " 34. CBH_m_av\n",
      " 35. HigCC_p_av\n",
      " 36. LowCC_p_av\n",
      " 37. MidCC_p_av\n",
      " 38. TotCC_p_av\n",
      " 39. BLH_m_av\n",
      " 40. Cape_av\n",
      " 41. Cin_av\n",
      " 42. sW_7_av\n",
      " 43. sW_28_av\n",
      " 44. sW_100_av\n",
      " 45. sW_289_av\n",
      " 46. gp_m2s2_av\n",
      " 47. t_2m_C_av\n",
      " 48. d_2m_C_av\n",
      " 49. sP_hPa_av\n",
      " 50. wv10_kh_av\n",
      " 51. wdir10_av\n",
      " 52. wv100_k_av\n",
      " 53. wdir100_av\n",
      " 54. rh_2m_av\n",
      " 55. VPD_Pa_av\n",
      " 56. dfmc_av\n",
      " 57. sW_1m_av\n",
      " 58. sW_3m_av\n",
      " 59. LCL_hPa_av\n",
      " 60. LCL_m_av\n",
      " 61. HDW_av\n",
      " 62. Haines_av\n",
      " 63. wSv_9_av\n",
      " 64. wSdir_9_av\n",
      " 65. wSv_7_av\n",
      " 66. wSdir_7_av\n",
      " 67. wSv_5_av\n",
      " 68. wSdir_5_av\n",
      " 69. wSv_1_av\n",
      " 70. wSdir_1_av\n",
      " 71. gT_s_9_av\n",
      " 72. gT_9_8_av\n",
      " 73. gT_8_7_av\n",
      " 74. gT_7_5_av\n",
      " 75. gT_5_3_av\n",
      " 76. CMLG_av\n",
      " 77. LFC_hPa_av\n",
      " 78. CCL_hPa_av\n",
      " 79. EL_m_av\n",
      " 80. VentIdx_av\n",
      " 81. LiftIdx_av\n",
      " 82. gp_950_av\n",
      " 83. gp_850_av\n",
      " 84. gp_700_av\n",
      " 85. gp_500_av\n",
      " 86. gp_300_av\n",
      " 87. rh_950_av\n",
      " 88. rh_850_av\n",
      " 89. rh_700_av\n",
      " 90. rh_500_av\n",
      " 91. rh_300_av\n",
      " 92. t_950_av\n",
      " 93. t_850_av\n",
      " 94. t_700_av\n",
      " 95. t_500_av\n",
      " 96. t_300_av\n",
      " 97. wv_950_av\n",
      " 98. wv_850_av\n",
      " 99. wv_700_av\n",
      "100. wv_500_av\n",
      "101. wv_300_av\n",
      "102. wdi_950_av\n",
      "103. wdi_850_av\n",
      "104. wdi_700_av\n",
      "105. wdi_500_av\n",
      "106. wdi_300_av\n",
      "107. vwv_950_av\n",
      "108. vwv_850_av\n",
      "109. vwv_700_av\n",
      "110. vwv_500_av\n",
      "111. vwv_300_av\n",
      "112. wv10_av\n",
      "113. wv100_av\n",
      "114. BLH_m_rt\n",
      "115. Recirc\n",
      "116. CircVar\n",
      "117. geometry\n",
      "118. ros_p_lg1\n",
      "119. f_start\n",
      "120. fire_rank\n"
     ]
    }
   ],
   "source": [
    "for i, coluna in enumerate(shp_processed.columns, 1):\n",
    "    print(f\"{i:3d}. {coluna}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0795c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_processed['HigCC_p_av'] = shp_processed['HigCC_p_av'] * 100\n",
    "shp_processed['MidCC_p_av'] = shp_processed['MidCC_p_av'] * 100\n",
    "shp_processed['LowCC_p_av'] = shp_processed['LowCC_p_av'] * 100\n",
    "shp_processed['TotCC_p_av'] = shp_processed['TotCC_p_av'] * 100\n",
    "\n",
    "shp_processed['sW_1m_av'] = shp_processed['sW_1m_av'] * 100\n",
    "shp_processed['sW_3m_av'] = shp_processed['sW_3m_av'] * 100\n",
    "shp_processed['sW_7_av'] = shp_processed['sW_7_av'] * 100\n",
    "shp_processed['sW_28_av'] = shp_processed['sW_28_av'] * 100\n",
    "shp_processed['sW_100_av'] = shp_processed['sW_100_av'] * 100\n",
    "shp_processed['sW_289_av'] = shp_processed['sW_289_av'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a47ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_processed['sdate'] = shp_processed['sdate'].astype(str)\n",
    "shp_processed['edate'] = shp_processed['edate'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ac33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(r'../../Data/Processed/PT-FireSprd_v2.1/L2_FireBehavior', exist_ok=True)\n",
    "shp_processed.to_file(r'../../Data/Processed/PT-FireSprd_v2.1/L2_FireBehavior/PT-FireProg_v2.1_L2_final.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
