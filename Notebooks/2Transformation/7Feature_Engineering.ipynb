{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c700a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8814c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de colunas: 116\n",
      "  1. fid\n",
      "  2. fname\n",
      "  3. year\n",
      "  4. id\n",
      "  5. type\n",
      "  6. sdate\n",
      "  7. edate\n",
      "  8. inidoy\n",
      "  9. enddoy\n",
      " 10. source\n",
      " 11. zp_link\n",
      " 12. burn_perio\n",
      " 13. area\n",
      " 14. growth_rat\n",
      " 15. ros_i\n",
      " 16. ros_p\n",
      " 17. spdir_i\n",
      " 18. spdir_p\n",
      " 19. int_i\n",
      " 20. int_p\n",
      " 21. duration_i\n",
      " 22. duration_p\n",
      " 23. qc\n",
      " 24. 1_3y_fir_p\n",
      " 25. 3_8y_fir_p\n",
      " 26. 8_ny_fir_p\n",
      " 27. elev_av\n",
      " 28. aspect_av\n",
      " 29. landform\n",
      " 30. fuel_model\n",
      " 31. f_load_av\n",
      " 32. land_use\n",
      " 33. land_use_d\n",
      " 34. CBH_m_av\n",
      " 35. HigCC_p_av\n",
      " 36. LowCC_p_av\n",
      " 37. MidCC_p_av\n",
      " 38. TotCC_p_av\n",
      " 39. BLH_m_av\n",
      " 40. Cape_av\n",
      " 41. Cin_av\n",
      " 42. gp_m2s2_av\n",
      " 43. t_2m_C_av\n",
      " 44. d_2m_C_av\n",
      " 45. sP_hPa_av\n",
      " 46. wv10_kh_av\n",
      " 47. wdir10_av\n",
      " 48. wv100_k_av\n",
      " 49. wdir100_av\n",
      " 50. rh_2m_av\n",
      " 51. VPD_Pa_av\n",
      " 52. dfmc_av\n",
      " 53. sW_1m_av\n",
      " 54. sW_3m_av\n",
      " 55. LCL_hPa_av\n",
      " 56. LCL_m_av\n",
      " 57. HDW_av\n",
      " 58. Haines_av\n",
      " 59. wSv_9_av\n",
      " 60. wSdir_9_av\n",
      " 61. wSv_7_av\n",
      " 62. wSdir_7_av\n",
      " 63. wSv_5_av\n",
      " 64. wSdir_5_av\n",
      " 65. wSv_1_av\n",
      " 66. wSdir_1_av\n",
      " 67. gT_s_9_av\n",
      " 68. gT_9_8_av\n",
      " 69. gT_8_7_av\n",
      " 70. gT_7_5_av\n",
      " 71. gT_5_3_av\n",
      " 72. CMLG_av\n",
      " 73. LFC_hPa_av\n",
      " 74. CCL_hPa_av\n",
      " 75. EL_m_av\n",
      " 76. VentIdx_av\n",
      " 77. LiftIdx_av\n",
      " 78. gp_950_av\n",
      " 79. gp_850_av\n",
      " 80. gp_700_av\n",
      " 81. gp_500_av\n",
      " 82. gp_300_av\n",
      " 83. rh_950_av\n",
      " 84. rh_850_av\n",
      " 85. rh_700_av\n",
      " 86. rh_500_av\n",
      " 87. rh_300_av\n",
      " 88. t_950_av\n",
      " 89. t_850_av\n",
      " 90. t_700_av\n",
      " 91. t_500_av\n",
      " 92. t_300_av\n",
      " 93. vwv_950_av\n",
      " 94. vwv_850_av\n",
      " 95. vwv_700_av\n",
      " 96. vwv_500_av\n",
      " 97. vwv_300_av\n",
      " 98. wv_950_av\n",
      " 99. wv_850_av\n",
      "100. wv_700_av\n",
      "101. wv_500_av\n",
      "102. wv_300_av\n",
      "103. wdi_950_av\n",
      "104. wdi_850_av\n",
      "105. wdi_700_av\n",
      "106. wdi_500_av\n",
      "107. wdi_300_av\n",
      "108. wv_10_av\n",
      "109. wdi_10_av\n",
      "110. wv_100_av\n",
      "111. wdi_100_av\n",
      "112. BLH_m_rt\n",
      "113. Recirc\n",
      "114. CircVar\n",
      "115. CircStd_dg\n",
      "116. geometry\n"
     ]
    }
   ],
   "source": [
    "shp = gpd.read_file(r\"..\\..\\Data\\Interim\\PT-FireSprd_v2.1\\L2_FireBehavior\\PT-FireProg_v2.1_L2_p_meteo.shp\")\n",
    "\n",
    "print(f\"Total de colunas: {len(shp.columns)}\")\n",
    "for i, coluna in enumerate(shp.columns, 1):\n",
    "    print(f\"{i:3d}. {coluna}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4110ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dias3\\AppData\\Local\\Temp\\ipykernel_9280\\373017451.py:6: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  shp_combined[\"sdate\"] = pd.to_datetime(shp_combined[\"sdate\"], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Lags temporais adicionados com base na feature ativa há X horas.\n"
     ]
    }
   ],
   "source": [
    "columns_to_lag = [\"ros_p\"]\n",
    "n_lags = 10\n",
    "shp_combined = shp.sort_values([\"fname\", \"zp_link\", \"sdate\"]).reset_index(drop=True)\n",
    "\n",
    "# garantir datetime\n",
    "shp_combined[\"sdate\"] = pd.to_datetime(shp_combined[\"sdate\"], errors='coerce')\n",
    "shp_combined[\"edate\"] = pd.to_datetime(shp_combined[\"edate\"], errors='coerce')\n",
    "\n",
    "for col in columns_to_lag:\n",
    "    if col not in shp_combined.columns:\n",
    "        print(f\"⚠️ Coluna '{col}' não encontrada. Pulando.\")\n",
    "        continue\n",
    "\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        lag_col_name = f\"{col}_lg{lag}\"\n",
    "        shp_combined[lag_col_name] = np.nan\n",
    "\n",
    "        # processa por grupo (fname, zplink)\n",
    "        for (fname, zplink), group in shp_combined.groupby([\"fname\", \"zp_link\"]):\n",
    "            group = group.sort_values(\"sdate\")\n",
    "            group_idx = group.index\n",
    "\n",
    "            for idx in group_idx:\n",
    "                current_time = shp_combined.loc[idx, \"sdate\"]  # início do evento atual\n",
    "                target_time = current_time - pd.Timedelta(hours=lag)\n",
    "\n",
    "                # procurar a feature anterior que estava ativa no instante target_time\n",
    "                mask = (group[\"sdate\"] <= target_time) & (group[\"edate\"] > target_time)\n",
    "\n",
    "                if mask.any():\n",
    "                    active_row = group.loc[mask].iloc[-1]\n",
    "                    shp_combined.at[idx, lag_col_name] = active_row[col]\n",
    "                else:\n",
    "                    shp_combined.at[idx, lag_col_name] = np.nan\n",
    "\n",
    "print(\"✅ Lags temporais adicionados com base na feature ativa há X horas.\")\n",
    "\n",
    "shp_lags = shp_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7b7904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying calculation for 'Agueda_08082016':\n",
      "              fname               sdate  duration\n",
      "70  Agueda_08082016 2016-08-08 04:00:00       0.0\n",
      "71  Agueda_08082016 2016-08-08 04:00:00       0.0\n",
      "72  Agueda_08082016 2016-08-08 12:30:00     510.0\n",
      "73  Agueda_08082016 2016-08-08 12:30:00     510.0\n",
      "74  Agueda_08082016 2016-08-10 13:30:00    3450.0\n",
      "75  Agueda_08082016 2016-08-10 13:30:00    3450.0\n",
      "76  Agueda_08082016 2016-08-10 13:30:00    3450.0\n",
      "49  Agueda_08082016                 NaT       NaN\n",
      "50  Agueda_08082016                 NaT       NaN\n",
      "51  Agueda_08082016                 NaT       NaN\n",
      "\n",
      "Verifying calculation for 'Gouveia_10082015':\n",
      "                 fname               sdate  duration\n",
      "1558  Gouveia_10082015 2015-08-10 14:30:00       0.0\n",
      "1559  Gouveia_10082015 2015-08-10 14:30:00       0.0\n",
      "1560  Gouveia_10082015 2015-08-11 03:00:00     750.0\n",
      "1561  Gouveia_10082015 2015-08-11 12:30:00    1320.0\n",
      "1562  Gouveia_10082015 2015-08-11 12:30:00    1320.0\n",
      "1563  Gouveia_10082015 2015-08-11 12:30:00    1320.0\n",
      "1564  Gouveia_10082015 2015-08-11 12:30:00    1320.0\n",
      "1555  Gouveia_10082015                 NaT       NaN\n",
      "1556  Gouveia_10082015                 NaT       NaN\n",
      "1557  Gouveia_10082015                 NaT       NaN\n"
     ]
    }
   ],
   "source": [
    "def calculate_fire_durations(df):\n",
    "    \"\"\"\n",
    "    Calculates 'duration' (time since fire start) and lag features \n",
    "    (time differences between consecutive observations).\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # 1. Convert sdate to datetime\n",
    "    df_temp['sdate'] = pd.to_datetime(df_temp['sdate'], errors='coerce')\n",
    "    \n",
    "    # 2. Sort by fire name and date\n",
    "    df_temp = df_temp.sort_values(by=['fname', 'sdate'])\n",
    "    \n",
    "    # 3. Calculate 'duration' (time since the start of the fire)\n",
    "    fire_start_times = df_temp.groupby('fname')['sdate'].transform('min')\n",
    "    df_temp['f_start'] = (df_temp['sdate'] - fire_start_times).dt.total_seconds() / 60\n",
    "    \n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "# Apply the corrected function\n",
    "shp_processed = calculate_fire_durations(shp_lags)\n",
    "\n",
    "# --- Verification ---\n",
    "print(\"\\nVerifying calculation for 'Agueda_08082016':\")\n",
    "print(shp_processed[shp_processed['fname'] == 'Agueda_08082016'][\n",
    "    ['fname', 'sdate', 'duration']\n",
    "].head(10))\n",
    "\n",
    "print(\"\\nVerifying calculation for 'Gouveia_10082015':\")\n",
    "print(shp_processed[shp_processed['fname'] == 'Gouveia_10082015'][\n",
    "    ['fname', 'sdate', 'duration']\n",
    "].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4893de57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dias3\\AppData\\Local\\Temp\\ipykernel_9280\\2298485294.py:22: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  s = row['sdate'].ceil('H')  # next full hour\n",
      "C:\\Users\\dias3\\AppData\\Local\\Temp\\ipykernel_9280\\2298485294.py:23: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  e = row['edate'].floor('H') # last full hour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire front rank distribution:\n",
      "fire_rank\n",
      "-3       3\n",
      "-2      14\n",
      "-1    1640\n",
      " 0    1020\n",
      " 1     339\n",
      " 2     339\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total rows: 3355\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create fire front ranking system:\n",
    "    -3: Other missing values\n",
    "    -2: No meteorological data because of short time interval\n",
    "    -1: Missing sdate or edate\n",
    "    0: Single fire front for specific fire at specific time\n",
    "    1: Multiple fire fronts - highest ROS_p\n",
    "    2: Multiple fire fronts - lowest ROS_p\n",
    "\"\"\"\n",
    "shp_processed['fire_rank'] = 0\n",
    "\n",
    "# Handle missing dates\n",
    "missing_mask = shp_processed['sdate'].isna() | shp_processed['edate'].isna()\n",
    "shp_processed.loc[missing_mask, 'fire_rank'] = -1\n",
    "\n",
    "\n",
    "# Handle intervals that don't contain exact hour (-2)\n",
    "def interval_has_full_hour(row):\n",
    "    if pd.isna(row['sdate']) or pd.isna(row['edate']):\n",
    "        return False\n",
    "    # Generate range of hours\n",
    "    s = row['sdate'].ceil('H')  # next full hour\n",
    "    e = row['edate'].floor('H') # last full hour\n",
    "    return s <= e\n",
    "\n",
    "mask_no_full_hour = (~missing_mask) & (~shp_processed.apply(interval_has_full_hour, axis=1))\n",
    "shp_processed.loc[mask_no_full_hour, 'fire_rank'] = -2\n",
    "\n",
    "# Handle missing meteorological data (-3)\n",
    "mask_missing_meteo = shp_processed['t_2m_C_av'].isna() & (shp_processed['ros_p'] != -1) & (shp_processed['fire_rank'] != -2)\n",
    "shp_processed.loc[mask_missing_meteo, 'fire_rank'] = -3\n",
    "\n",
    "# Process valid rows\n",
    "valid_mask = (~missing_mask) & (~mask_no_full_hour)\n",
    "for (fname, sdate), group in shp_processed[valid_mask].groupby(['fname', 'sdate']):\n",
    "    valid_rows = group[(group['type'] == 'p') & (group['ros_p'] > 0) & (group['fire_rank'] == 0)]\n",
    "    if len(valid_rows) > 1:\n",
    "        max_ros_idx = valid_rows['ros_p'].idxmax()\n",
    "        min_ros_idx = valid_rows['ros_p'].idxmin()\n",
    "        shp_processed.loc[max_ros_idx, 'fire_rank'] = 1\n",
    "        shp_processed.loc[min_ros_idx, 'fire_rank'] = 2\n",
    "\n",
    "# Display results\n",
    "rank_counts = shp_processed['fire_rank'].value_counts().sort_index()\n",
    "print(\"Fire front rank distribution:\")\n",
    "print(rank_counts)\n",
    "print(f\"\\nTotal rows: {len(shp_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cf40b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. fid\n",
      "  2. fname\n",
      "  3. year\n",
      "  4. id\n",
      "  5. type\n",
      "  6. sdate\n",
      "  7. edate\n",
      "  8. inidoy\n",
      "  9. enddoy\n",
      " 10. source\n",
      " 11. zp_link\n",
      " 12. burn_perio\n",
      " 13. area\n",
      " 14. growth_rat\n",
      " 15. ros_i\n",
      " 16. ros_p\n",
      " 17. spdir_i\n",
      " 18. spdir_p\n",
      " 19. int_i\n",
      " 20. int_p\n",
      " 21. duration_i\n",
      " 22. duration_p\n",
      " 23. qc\n",
      " 24. 1_3y_fir_p\n",
      " 25. 3_8y_fir_p\n",
      " 26. 8_ny_fir_p\n",
      " 27. elev_av\n",
      " 28. aspect_av\n",
      " 29. landform\n",
      " 30. fuel_model\n",
      " 31. f_load_av\n",
      " 32. land_use\n",
      " 33. land_use_d\n",
      " 34. CBH_m_av\n",
      " 35. HigCC_p_av\n",
      " 36. LowCC_p_av\n",
      " 37. MidCC_p_av\n",
      " 38. TotCC_p_av\n",
      " 39. BLH_m_av\n",
      " 40. Cape_av\n",
      " 41. Cin_av\n",
      " 42. gp_m2s2_av\n",
      " 43. t_2m_C_av\n",
      " 44. d_2m_C_av\n",
      " 45. sP_hPa_av\n",
      " 46. wv10_kh_av\n",
      " 47. wdir10_av\n",
      " 48. wv100_k_av\n",
      " 49. wdir100_av\n",
      " 50. rh_2m_av\n",
      " 51. VPD_Pa_av\n",
      " 52. dfmc_av\n",
      " 53. sW_1m_av\n",
      " 54. sW_3m_av\n",
      " 55. LCL_hPa_av\n",
      " 56. LCL_m_av\n",
      " 57. HDW_av\n",
      " 58. Haines_av\n",
      " 59. wSv_9_av\n",
      " 60. wSdir_9_av\n",
      " 61. wSv_7_av\n",
      " 62. wSdir_7_av\n",
      " 63. wSv_5_av\n",
      " 64. wSdir_5_av\n",
      " 65. wSv_1_av\n",
      " 66. wSdir_1_av\n",
      " 67. gT_s_9_av\n",
      " 68. gT_9_8_av\n",
      " 69. gT_8_7_av\n",
      " 70. gT_7_5_av\n",
      " 71. gT_5_3_av\n",
      " 72. CMLG_av\n",
      " 73. LFC_hPa_av\n",
      " 74. CCL_hPa_av\n",
      " 75. EL_m_av\n",
      " 76. VentIdx_av\n",
      " 77. LiftIdx_av\n",
      " 78. gp_950_av\n",
      " 79. gp_850_av\n",
      " 80. gp_700_av\n",
      " 81. gp_500_av\n",
      " 82. gp_300_av\n",
      " 83. rh_950_av\n",
      " 84. rh_850_av\n",
      " 85. rh_700_av\n",
      " 86. rh_500_av\n",
      " 87. rh_300_av\n",
      " 88. t_950_av\n",
      " 89. t_850_av\n",
      " 90. t_700_av\n",
      " 91. t_500_av\n",
      " 92. t_300_av\n",
      " 93. vwv_950_av\n",
      " 94. vwv_850_av\n",
      " 95. vwv_700_av\n",
      " 96. vwv_500_av\n",
      " 97. vwv_300_av\n",
      " 98. wv_950_av\n",
      " 99. wv_850_av\n",
      "100. wv_700_av\n",
      "101. wv_500_av\n",
      "102. wv_300_av\n",
      "103. wdi_950_av\n",
      "104. wdi_850_av\n",
      "105. wdi_700_av\n",
      "106. wdi_500_av\n",
      "107. wdi_300_av\n",
      "108. wv_10_av\n",
      "109. wdi_10_av\n",
      "110. wv_100_av\n",
      "111. wdi_100_av\n",
      "112. BLH_m_rt\n",
      "113. Recirc\n",
      "114. CircVar\n",
      "115. CircStd_dg\n",
      "116. geometry\n",
      "117. ros_p_lg1\n",
      "118. ros_p_lg2\n",
      "119. ros_p_lg3\n",
      "120. ros_p_lg4\n",
      "121. ros_p_lg5\n",
      "122. ros_p_lg6\n",
      "123. ros_p_lg7\n",
      "124. ros_p_lg8\n",
      "125. ros_p_lg9\n",
      "126. ros_p_lg10\n",
      "127. duration\n",
      "128. fire_rank\n"
     ]
    }
   ],
   "source": [
    "for i, coluna in enumerate(shp_processed.columns, 1):\n",
    "    print(f\"{i:3d}. {coluna}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795c497",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_processed['HigCC_p_av'] = shp_processed['HigCC_p_av'] * 100\n",
    "shp_processed['MidCC_p_av'] = shp_processed['MidCC_p_av'] * 100\n",
    "shp_processed['LowCC_p_av'] = shp_processed['LowCC_p_av'] * 100\n",
    "shp_processed['TotCC_p_av'] = shp_processed['TotCC_p_av'] * 100\n",
    "\n",
    "shp_processed['sW_1m_av'] = shp_processed['sW_1m_av'] * 100\n",
    "shp_processed['sW_3m_av'] = shp_processed['sW_3m_av'] * 100\n",
    "shp_processed['sW_7_av'] = shp_processed['sW_7_av'] * 100\n",
    "shp_processed['sW_28_av'] = shp_processed['sW_28_av'] * 100\n",
    "shp_processed['sW_100_av'] = shp_processed['sW_100_av'] * 100\n",
    "shp_processed['sW_289_av'] = shp_processed['sW_289_av'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57a47ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_processed['sdate'] = shp_processed['sdate'].astype(str)\n",
    "shp_processed['edate'] = shp_processed['edate'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44ac33ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(r'..\\..\\Data\\Processed\\PT-FireSprd_v2.1\\L2_FireBehavior', exist_ok=True)\n",
    "shp_processed.to_file(r'..\\..\\Data\\Processed\\PT-FireSprd_v2.1\\L2_FireBehavior\\PT-FireProg_v2.1_L2_final.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
